---
title: 'P&S-2022: Lab assignment 2'
author: "Name1, Name2, Name3"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with
    explanations + oral defense). Submission deadline **November 1,
    2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to
    **cms** both the source *R notebook* **and** the generated html
    file\
-   At the beginning of the notebook, provide a work-breakdown structure
    estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer
        to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is
        just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you
        use to complete the task) as well as histograms etc to
        illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding
        theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree
        with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit**
    ordinal number of your team on the list. Include the line
    **set.seed(team id number)** at the beginning of your code to make
    your calculations reproducible. Also observe that the answers **do**
    depend on this number!\
-   Take into account that not complying with these instructions may
    result in point deduction regardless of whether or not your
    implementation is correct.

Work Breakdown Structure: - Task 1 (Hamming Code): Oleksii (Problem
analysis, simulation logic, and R implementation) - Task 2 (Poisson
Distribution): Anton (Theoretical calculations for $\mu$, R
implementation for CLT, simulation) - Task 3 (Exponential Distribution):
Anton (Theoretical calculations for $N$, R implementation for CLT,
simulation) - Task 4 (Moments & Independence): Anastasiia (4.1 and 4.2
tasks)
------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it
    to a $7$-bit *codeword*
    $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where
    $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the
    received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome
    vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$
    *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary
    $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no.
    $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or
    more than one), while $(1 1 0 )$ means the third bit (or more than
    one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in
    $\mathbf{r}$ to get the corrected
    $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and
    find the estimate $\hat p$ of the probability $p^*$ of correct
    transmission of a single message $\mathbf{m}$. Comment why, for
    large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator
    of success by the standard error of your sample and using the CLT,
    predict the \emph{confidence} interval
    $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate
    $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while
    transmitting a $4$-digit binary message. Do you think it is one of
    the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# our team id number 
id <- 49

set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
# cat("The matrix G is: \n") 
#G  
#cat("The matrix H is: \n") 
#H
#cat("The product GH must be zero: \n")
#(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages
N <- 10000 # Number of simulations

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(N)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
# generating a 7-bit error vector for each of the N messages
errors <- matrix(sample(c(0, 1), 7 * N, replace = TRUE, prob = c(1 - p, p)), nrow = N)

# Add errors to the codewords to get the received messages
received <- (codewords + errors) %% 2 
```

The next steps include detecting the errors in the received messages,
correcting them, and then decoding the obtained messages. After this,
you can continue with calculating all the quantities of interest

```{r}
syndrome <- (received %*% H) %% 2

# Convert the binary syndrome vector to an integer (0-7)
# This integer is the position of the error bit
# z = (z1 z2 z3), position = z1*1 + z2*2 + z3*4
error_pos <- syndrome %*% c(1, 2, 4)

# Correct the errors
# We create a copy of the received messages to store the corrected versions
r_star <- received
for (i in 1:N) {
  pos <- error_pos[i]
  if (pos > 0) {
    # If pos > 0, an error was detected at that bit.
    # We flip the bit (add 1 mod 2)
    r_star[i, pos] <- (r_star[i, pos] + 1) %% 2
  }
}

# Decode the corrected message r*
# m* = (r*_3, r*_5, r*_6, r*_7)
m_star <- r_star[, c(3, 5, 6, 7)]

# We compare the original messages with the decoded messages and
# check which messages were transmitted correctly
successes <- rowSums(abs(messages - m_star)) == 0
p_hat <- mean(successes)

cat("Number of simulations (N):", N, "\n")
cat("Estimated probability of correct transmission (p_hat):", p_hat, "\n")

# Calculate the 95% confidence interval
# Standard error of the estimate
se <- sqrt(p_hat * (1 - p_hat) / N)
# For 95% CI, z-score is ~1.96
epsilon <- 1.96 * se
ci_low <- p_hat - epsilon
ci_high <- p_hat + epsilon

cat("Standard Error (se):", se, "\n")
cat("Half-length (epsilon):", epsilon, "\n")
cat("95% Confidence Interval for p*:", paste0("(", ci_low, ", ", ci_high, ")\n"))

# Find N that guarantees epsilon <= 0.03
N_req <- (1.96 / 0.03)^2 * p_hat * (1 - p_hat)
# If we want to see  worst-case variance , we can use p*=0.5
N_req_conservative <- (1.96 / 0.03)^2 * 0.5 * 0.5

cat("Required N (using p_hat):", ceiling(N_req), "\n")
cat("Required N (conservative):", ceiling(N_req_conservative), "\n")

# calculating the number of errors in the decoded message
num_errors <- rowSums(abs(messages - m_star))


print(table(num_errors))

# Draw the histogram
hist(num_errors,
     breaks = seq(-0.5, 4.5, by = 1),
     probability = TRUE,
     main = "Histogram of Errors in Decoded Message",
     xlab = "Number of Errors (k)",
     col = "lightblue")
```

**Summary and Conclusions** This simulation checks the reliability of a
[7,4] Hamming code on a very noisy communication channel. We assume each
transmitted bit has a high probability, $p = 0.49$, of being errored.
The code's goal is to detect and correct these errors to recover the
original 4-bit message.

After running $N=10,000$ simulations, we find the estimated probability
of a correct transmission (p_hat). This is the fraction of the 10,000
messages that were decoded perfectly. Our simulation shows this value is
very low +- 7%. This low success rate is expected: the [7,4] Hamming
code is only designed to correct a single bit error. With a 49% bit
error probability, the chance of a 7-bit codeword having 0 or 1 error is
tiny, while the chance of having 2 or more errors (which the code cannot
fix) is extremely high.

The code then uses the Central Limit Theorem to construct a 95%
confidence interval around this estimate. This provides a statistically
sound range (something like 6.5% to 7.7%) where the true success
probability of this system likely lies. We also calculate the number of
trials (N) that would be needed to guarantee our estimate is accurate to
within $\pm 3\%$.

Finally, the histogram of decoded errors gives us a visual insight. It
shows the distribution of the number of errors (0 to 4) in the 4-bit
message. We observe that almost all failed decodings (the \~93% of
cases) result in 3 or 4 errors. This demonstrates that when the Hamming
code fails (due to 2 or more transmission errors), it fails really bad,
completely mixing the message rather than just be slightly off.

In conclusion, the simulation confirms the principles of the Law of
Large Numbers (in estimating p_hat) and the Central Limit Theorem (in
finding the confidence interval). It also clearly shows that while the
[7,4] Hamming code is a clever design, it is entirely unsuitable for a
channel with such a high error rate, as it fails to correctly transmit
the message over 90% of the time.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a
Poisson distribution. As you remember, a Poisson random variable
describes occurrences of rare events, i.e., counts the number of
successes in a large number of independent random experiments. One of
the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big
*half-life period* $T$; it is vitally important to know the probability
that during a one second period, the number of nuclei decays will not
exceed some critical level $k$. This probability can easily be estimated
using the fact that, given the *activity* ${\lambda}$ of the element
(i.e., the probability that exactly one nucleus decays in one second)
and the number $N$ of atoms in the sample, the random number of decays
within a second is well modelled by Poisson distribution with parameter
$\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms
is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro
constant, and $M$ is the molar (atomic) mass of the element. The
activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is
measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive
element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life
period $T = 30.1$ years and mass
$m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by
$X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays
in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll
    need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$
    gets very close to a normal one as $n$ becomes large and identify
    that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical
        cumulative distribution function $\hat F_{\mathbf{s}}$ of
        $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$
        of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to
        visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.\
3.  Calculate the largest possible value of $n$, for which the total
    number of decays in one second is less than $8 \times 10^8$ with
    probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality,
        Chernoff bound and Central Limit Theorem, and compare the
        results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less
        than critical value ($8 \times 10^8$) and calculate the
        empirical probability; comment whether it is close to the
        desired level $0.95$

```{r}
set.seed(49)

team_id <- 49 
m <- team_id * 1e-6 # sample mass in grams
M <- 136.907        # molar mass of Cs-137 (g/mol)
N_A <- 6e23         # Avogadro constant
T_years <- 30.1     # half-life in years

T_sec <- T_years * 365.25 * 24 * 3600    # convert to seconds
lambda <- log(2) / T_sec                 # decay probability per atom per second
N <- (m / M) * N_A                       # number of atoms in one sample
mu <- N * lambda                         # Poisson parameter 
cat("lambda =", lambda, "\nN =", N, "\nmu =", mu, "\n")
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- mu
n <- 5      # sample size
sigma <- sqrt(mu / n)
K <- 1e3    # number of repetitions
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow = n))
cat("Normal approximation: mean =", mu, " sd =", sigma, "\n")
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu - 3*sigma, mu + 3*sigma)
Fs <- ecdf(sample_means)

plot(Fs,
     xlim = xlims,
     ylim = c(0, 1),
     col = "blue",
     lwd = 2,
     main = paste("Comparison of ECDF and Normal CDF (n =", n, ")"),
     xlab = "Sample mean", ylab = "Cumulative probability")

curve(pnorm(x, mean = mu, sd = sigma),
      col = "red", lwd = 2, add = TRUE)

legend("topleft", legend = c("Empirical CDF", "Normal CDF"),
       col = c("blue", "red"), lwd = 2)
```

```{r}
critical <- 8e8   # critical total decays
K <- 10000        # number of Monte Carlo repetitions


n_markov <- floor(0.05 * critical / mu)
cat("Markov bound: n <=", n_markov, "\n")

chernoff_bound <- function(n) {
  lambda_sum <- n * mu
  delta <- critical / lambda_sum - 1
  bound <- exp(-lambda_sum * ((1 + delta) * log(1 + delta) - delta))
  return(bound)
}

n_try <- 1
while(chernoff_bound(n_try) > 0.05) {
  n_try <- n_try + 1
}
n_chernoff <- n_try
cat("Chernoff bound: n <=", n_chernoff, "\n")

z <- qnorm(0.95)
clt_func <- function(n) {
  (critical - n*mu)/sqrt(n*mu) - z
}
n_clt <- uniroot(clt_func, c(1, 1e6))$root
cat("CLT bound: n ≈", floor(n_clt), "\n")
```

```{r}
n_sim <- floor(n_clt)  # pick a safe n

S <- replicate(K, sum(rpois(n_sim, mu)))

empirical_prob <- mean(S < critical)
hist(S, breaks=50, probability=TRUE,
     main=paste("Histogram of total decays for n =", n_sim),
     xlab="Total decays in 1 second", col="lightgreen")
abline(v=critical, col="red", lwd=2, lty=2)
```

Summary:

We analyzed the radioactive decay of Cesium-137 samples using a Poisson
model to represent the number of nuclei decaying per second. Simulations
of multiple independent samples showed that the distribution of the
sample means closely follows a normal distribution, in agreement with
the Central Limit Theorem.

We also estimated the maximum number of samples for which the total
decays remain below a critical threshold 8*10^8 with at least 95%
probability. Theoretical bounds using Markov, Chernoff, and CLT
approximations were computed and validated through simulation, with the
empirical probability closely matching the desired safety level.

The results highlight that even microgram-level samples produce
extremely high decay counts, the Poisson model effectively captures this
randomness, and normal approximations provide practical estimates for
safety limits in laboratory settings.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of
    $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as
    $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the
        \textbf{r.v.} $X_i$ and calculate the sample mean
        $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the
        \emph{empirical cumulative distribution} function
        $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of
        $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph
        to visualize their proximity;\
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.
2.  The place can be considered safe when the number of clicks in one
    minute does not exceed $100$. It is known that the parameter $\nu$
    of the resulting exponential distribution is proportional to the
    number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where
    $\nu_1$ is the parameter for one sample. Determine the maximal
    number of radioactive samples that can be stored in that place so
    that, with probability $0.95$, the place is identified as safe. To
    do this,
    -   express the event of interest in terms of the \textbf{r.v.}
        $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov
        inequality, Chernoff bound and Central Limit Theorem and compare
        the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization
        $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum
        $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the
        $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe
        and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
id <- 49
nu1 <- id + 10
rate1 <- 1 / nu1

K <- 1000
n_values <- c(5, 10, 50) 

# Set up plot area for 3 plots
par(mfrow = c(1, 3))

# this loop runs the simulation for each n in n_values
for (n in n_values) {
  # generate K sample means, each from a sample of size n
  sample_means <- colMeans(matrix(rexp(n * K, rate = rate1), nrow = n))

  # theoretical mean and standard deviation for the bell curve (from CLT)
  mu_normal <- nu1
  sigma_normal <- sqrt(nu1^2 / n)

  # plot the cdf from our simulated data (the ecdf)
  plot(ecdf(sample_means),
       col = "blue",
       lwd = 2,
       main = paste("ECDF vs. Normal CDF (n =", n, ")"),
       xlab = "Sample Mean", ylab = "Probability")

  # overlay the perfect theoretical bell curve cdf
  curve(pnorm(x, mean = mu_normal, sd = sigma_normal), 
        col = "red", lwd = 2, add = TRUE)
  
  # we check how far apart the blue and red lines are
  x_grid <- seq(mu_normal - 4*sigma_normal, mu_normal + 4*sigma_normal, length.out=500)
  max_diff <- max(abs(ecdf(sample_means)(x_grid) - pnorm(x_grid, mean = mu_normal, sd = sigma_normal)))
  
  cat(paste("For n =", n, ", the max difference is:", round(max_diff, 4), "\n"))
}

# reset the plot grid
par(mfrow = c(1, 1))
```
As you can see from the plots, when the sample size n is small (like $n=5$), the blue line (our data) doesn't perfectly match the red line. But as n gets bigger, the blue line gets more like red line. The "max difference" we printed gets smaller and smaller. So it is exactly what CLT says: the larger the sample size, the more the distribution of the sample means looks like a perfect normal distribution.


#### Next, calculate the parameters of the standard normal approximation

```{r}
# This code assumes nu1=59 from the previous chunk
# and that we are plotting for n=50, the last value in the loop.
n <- 50
mu <- nu1 
sigma <- nu1 / sqrt(n)

# You can print them to check
cat("Plotting for n =", n, "\n")
cat("Theoretical Mean:", mu, "\n")
cat("Theoretical Standard deviation - sigma:", sigma, "\n")
```

#### We can now plot ecdf and cdf

```{r}
# Set the plot limits to 3 standard deviations around the mean
xlims <- c(mu - 3*sigma, mu + 3*sigma)

# Get the ecdf function from our simulated data
Fs <- ecdf(sample_means)

# Plot the ECDF (blue step-plot)
plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ECDF and Normal CDF (n=50)",
     xlab = "Sample Mean",
     ylab = "Cumulative Probability")

# Add the theoretical Normal CDF (red smooth curve)
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)

legend("bottomright", legend = c("Empirical CDF (n=50)", "Theoretical Normal CDF"),
       col = c("blue", "red"), lwd = 2)
```

#### Safety Calculation
Here, we need to find the maximum number of samples, N, that can be safely stored.The "safe" condition is: "number of clicks in one minute (60 seconds) does not exceed 100."Let Y be the number of clicks in 60 seconds.From Part 1, the mean time for one sample is nu1 = 59. The rate is rate1 = 1/nu1.The prompt states the rate is proportional to N, so the total rate is rate_N = N * rate1 = N / 59 clicks/sec.Y, the number of clicks in 60 seconds, follows a Poisson distribution:Y \sim \text{Poisson}(\text{mu} = 60 * rate_N = 60 * N / 59).We want to find the largest integer N such that P(Y \le 100) \ge 0.95$.

1. Markov's Inequality: $P(Y > 100) \le E[Y] / 100$.
We need $E[Y] / 100 \le 0.05 \implies E[Y] \le 5$.$\text{mu} = 60 \times N / 59 \le 5 \implies N \le (5 \times 59) / 60 \approx 4.91$.
So, Markov's bound suggests $N \le 4$.

2. CLT (Poisson Approximation): Since $\text{mu}$ will be large, we can approximate the Poisson $Y$ as a Normal distribution: $Y \approx \text{Normal}(\text{mu}, \text{variance} = \text{mu})$.
We want $P(Y \le 100) \ge 0.95$. Using continuity correction: $P(Y \le 100.5) \ge 0.95$.
Standardizing: $P(Z \le (100.5 - \text{mu}) / \text{sqrt}(\text{mu})) \ge 0.95$.
The z-score for 0.95 is qnorm(0.95) which is approx 1.645.
$(100.5 - \text{mu}) / \text{sqrt}(\text{mu}) \ge 1.645 \implies 100.5 - \text{mu} \ge 1.645 \times \text{sqrt}(\text{mu})$.
Let $x = \text{sqrt}(\text{mu})$. We get $x^2 + 1.645x - 100.5 \le 0$.
Solving $x^2 + 1.645x - 100.5 = 0$ gives $x \approx 9.21$.
So $\text{mu} \le 9.21^2 \approx 84.8$.
$60 \times N / 59 \le 84.8 \implies N \le (84.8 \times 59) / 60 \approx 83.3$.
The CLT bound suggests $N \le 83$.



Now we can simulate the process with $N=83$ and check the probability. $S_{100} = X_1 + \dots + X_{100}$. The event "at most 100 clicks in 60s" ($Y \le 100$) is almostt the same as "the time to the 100th click is 60s or more" ($S_{100} \ge 60$). We will simulate $S_{100}$ and check this.
```{r}
# Use N=83 based on CLT(Poisson) bound
N_clt <- 83
nu_N <- nu1 * N_clt
rate_N <- N_clt / nu1

K_sim <- 10000
# we simulate the time to the 100th click, S_100
# S_100 is a sum of 100 exponentials, which is Gamma(shape=100, rate=rate_N)
s_samples <- rgamma(K_sim, shape = 100, rate = rate_N)

# We check the probability P(S_100 >= 60)
# it is basically the same as "100 clicks took at least 60 seconds"
# what is "at most 100 clicks in 60s".
prob_safe <- mean(s_samples >= 60)

cat("Simulation for N =", N_clt, "\n")
cat("Rate per second (lambda_N):", rate_N, "\n")
cat("Expected time for 100 clicks (E[S_100]):", 100 / rate_N, "sec\n")
cat("Empirical probability of S_100 >= 60 sec:", prob_safe, "\n")

hist(s_samples, breaks=50, probability=TRUE,
     main=paste("Histogram of S_100 (Time to 100 Clicks) for N =", N_clt),
     xlab="Time (seconds)", col="lightblue")
abline(v=60, col="red", lwd=2, lty=2)
legend("topright", "Safety Threshold (60s)", col="red", lty=2, lwd=2)
```


So, in this task, we first proved that the Central Limit Theorem works. We saw in Part 1 that even when we started with the weird, exponential distribution, the averages of our samples started to look like a normal. Our plots for $n=50$ matched the theoretical red line almost perfectly, which was cool to see.In Part 2, we used this exact same idea to solve the safety problem. We figured out that the number of clicks in one minute (a Poisson distribution) would also act like a bell curve because the numbers were so big. This let us calculate the max number of samples ($N$) that could be stored safely. Our CLT calculation gave the most realistic answer ($N=84$), and our final simulation proved it was correct, giving us almost exactly the 95% safety probability we were looking for. It shows that the CLT is super useful for approximating problems like this.

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its
    moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why
        $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$
        and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations
        $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of
        $Y := \frac{1}{X}$ to calculate the values of
        $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$.
        Comment on the received results;

```{r}
teamidnumber <- 49
mu <- teamidnumber
sigma <- sqrt(2*teamidnumber + 7)
N <- 100

set.seed(123)
X <- rnorm(N, mean=mu, sd=sigma)
Y <- 1 / X

inv_mean_X <- 1 / mean(X)
mean_Y <- mean(Y)

inv_mean_X
mean_Y


```

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with
        parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the
        Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the
        Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain
        the results. Comment on the difference of relations between the
        pairs of random variables. Which pair of r.v.'s is dependent and
        which one is similar?

```{r}
set.seed(123)
X <- rexp(100, rate=2)
Y <- rexp(100, rate=2)
Z <- log(X) + 5

qqplot(X, Y); abline(0,1, col="red")
plot(X, Y, main="Scatterplot X vs Y")

qqplot(X, Z); abline(0,1, col="blue")
plot(X, Z, main="Scatterplot X vs Z")
```

    ------------------------------------------------------------------------

2.  You toss a fair coin three times and a random variable $X$ records
    how many times the coin shows Heads. You convince your friend that
    they should play a game with the following payoff: every round
    (equivalent to three coin tosses) will cost £$1$. They will receive
    £$0.5$ for every coin showing Heads. What is the expected value and
    the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

    2.  What are the expected value and variance of X? Simulate
        realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the
        values of sample mean $\overline{\mathbf{X}}$ and sample
        variance
        $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$.
        Comment on the results;

```{r}
set.seed(123)
X <- rbinom(100, size=3, prob=0.5)

mean_X <- mean(X)
var_X <- var(X)

mean_X
var_X
```

    3.  What are the expected value and variance of Y? Simulate
        realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the
        values of sample mean $\overline{\mathbf{Y}}$ and sample
        variance
        $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$.
        Comment on the results;

```{r}
Y <- 0.5*X - 1

mean_Y <- mean(Y)
var_Y <- var(Y)

mean_Y
var_Y
```

**Summary and Conclusions**

This task explored properties of random variables and their moments. Simulations confirmed that E(1/X) ≠ 1/E(X) for a normal variable. QQ-plots and scatterplots showed that independent variables X and Y are uncorrelated, while X and Z = log(X)+5 are dependent.

For the coin-toss game, X is binomial and the sample mean and variance of X and Y = 0.5X-1 closely match theoretical values. The expected payoff is negative, showing the game is disadvantageous. Overall, the results illustrate the Law of Large Numbers, functional dependence, and behavior of expectations and variances under transformations.

------------------------------------------------------------------------

### General summary and conclusions

In this lab, we successfully applied core probability concepts using R.

- Task 1: We simulated a [7,4] Hamming code, confirming its $\approx 7\%$ success rate on a high-noise ($p=0.49$) channel, proving it bad for such tasks.

- Task 2: We modeled Poisson radioactive decay, verified the CLT for sample means, and found the CLT provided the tightest theoretical bound for safe sample storage.

- Task 3: We analyzed exponential Geiger clicks, again confirming the CLT, and used a Normal approximation to find a safe sample limit ($N=83$), which our simulation approved.

- Task 4: We demonstrated $E[1/X] \neq 1/E[X]$ (Jensen's inequality), used scatterplots to distinguish between independent and functionally dependent variables, and calculated the negative expected payoff of a Binomial game.

Overall, the lab provided practical experience using R to verify the LLN and CLT.
